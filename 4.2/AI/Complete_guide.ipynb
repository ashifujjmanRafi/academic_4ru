{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Hc85kZZa3S"
      },
      "source": [
        "Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj-rnvcoUBpT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV-P6nUdUd5H"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "x = np.random.random(1000)\n",
        "y = 3*x*x*x+7*x**2-12*x+2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RBCGyYmVNTP",
        "outputId": "8f4f3322-58d1-4ab8-8d83-276da50ca18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 32)                64        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 609\n",
            "Trainable params: 609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    Dense(32,activation='relu',input_shape=[1]),\n",
        "    Dense(16),\n",
        "    Dense(1)\n",
        "])\n",
        "model.summary()\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'mse',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U7YTpJUrWFiB",
        "outputId": "58f94cdf-275e-47df-a8bd-4d06c9a87c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/800\n",
            "32/32 [==============================] - 6s 3ms/step - loss: 1.7299 - accuracy: 0.0000e+00\n",
            "Epoch 2/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.0114 - accuracy: 0.0000e+00\n",
            "Epoch 3/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8721 - accuracy: 0.0000e+00\n",
            "Epoch 4/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8444 - accuracy: 0.0000e+00\n",
            "Epoch 5/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8216 - accuracy: 0.0000e+00\n",
            "Epoch 6/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8006 - accuracy: 0.0000e+00\n",
            "Epoch 7/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7848 - accuracy: 0.0000e+00\n",
            "Epoch 8/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7749 - accuracy: 0.0000e+00\n",
            "Epoch 9/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7670 - accuracy: 0.0000e+00\n",
            "Epoch 10/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7638 - accuracy: 0.0000e+00\n",
            "Epoch 11/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7647 - accuracy: 0.0000e+00\n",
            "Epoch 12/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7586 - accuracy: 0.0000e+00\n",
            "Epoch 13/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7591 - accuracy: 0.0000e+00\n",
            "Epoch 14/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7556 - accuracy: 0.0000e+00\n",
            "Epoch 15/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7550 - accuracy: 0.0000e+00\n",
            "Epoch 16/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7584 - accuracy: 0.0000e+00\n",
            "Epoch 17/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7554 - accuracy: 0.0000e+00\n",
            "Epoch 18/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7538 - accuracy: 0.0000e+00\n",
            "Epoch 19/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7478 - accuracy: 0.0000e+00\n",
            "Epoch 20/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7387 - accuracy: 0.0000e+00\n",
            "Epoch 21/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7255 - accuracy: 0.0000e+00\n",
            "Epoch 22/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7126 - accuracy: 0.0000e+00\n",
            "Epoch 23/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.0000e+00\n",
            "Epoch 24/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6711 - accuracy: 0.0000e+00\n",
            "Epoch 25/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6477 - accuracy: 0.0000e+00\n",
            "Epoch 26/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6238 - accuracy: 0.0000e+00\n",
            "Epoch 27/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5960 - accuracy: 0.0000e+00\n",
            "Epoch 28/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5647 - accuracy: 0.0000e+00\n",
            "Epoch 29/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5317 - accuracy: 0.0000e+00\n",
            "Epoch 30/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4973 - accuracy: 0.0000e+00\n",
            "Epoch 31/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 0.0000e+00\n",
            "Epoch 32/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.0000e+00\n",
            "Epoch 33/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3850 - accuracy: 0.0000e+00\n",
            "Epoch 34/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3428 - accuracy: 0.0000e+00\n",
            "Epoch 35/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.0000e+00\n",
            "Epoch 36/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2694 - accuracy: 0.0000e+00\n",
            "Epoch 37/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2334 - accuracy: 0.0000e+00\n",
            "Epoch 38/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2025 - accuracy: 0.0000e+00\n",
            "Epoch 39/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.0000e+00\n",
            "Epoch 40/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.0000e+00\n",
            "Epoch 41/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.0000e+00\n",
            "Epoch 42/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.0000e+00\n",
            "Epoch 43/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.0000e+00\n",
            "Epoch 44/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.0000e+00\n",
            "Epoch 45/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.0000e+00\n",
            "Epoch 46/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.0000e+00\n",
            "Epoch 47/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.0000e+00\n",
            "Epoch 48/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.0000e+00\n",
            "Epoch 49/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.0000e+00\n",
            "Epoch 50/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.0000e+00\n",
            "Epoch 51/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.0000e+00\n",
            "Epoch 52/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.0000e+00\n",
            "Epoch 53/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.0000e+00\n",
            "Epoch 54/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
            "Epoch 55/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 56/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 57/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 58/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 59/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 60/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.0000e+00\n",
            "Epoch 61/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 62/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 63/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 64/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 65/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 66/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 67/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.0000e+00\n",
            "Epoch 68/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 69/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
            "Epoch 70/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 71/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 72/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 73/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 74/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 75/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.0000e+00\n",
            "Epoch 76/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 77/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 78/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 79/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.0000e+00\n",
            "Epoch 80/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.0000e+00\n",
            "Epoch 81/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 82/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 83/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 84/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 85/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 86/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 87/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 88/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
            "Epoch 89/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.0000e+00\n",
            "Epoch 90/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 91/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 92/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.0000e+00\n",
            "Epoch 93/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 94/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.0000e+00\n",
            "Epoch 95/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 96/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.0000e+00\n",
            "Epoch 97/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 98/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.0000e+00\n",
            "Epoch 99/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.0000e+00\n",
            "Epoch 100/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 101/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 102/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 103/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 104/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 105/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 106/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.0000e+00\n",
            "Epoch 107/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
            "Epoch 108/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.0000e+00\n",
            "Epoch 109/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.0000e+00\n",
            "Epoch 110/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 111/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 112/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0451 - accuracy: 0.0000e+00\n",
            "Epoch 113/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 114/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 115/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 116/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 117/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.0000e+00\n",
            "Epoch 118/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 119/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 120/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 121/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 122/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.0000e+00\n",
            "Epoch 123/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.0000e+00\n",
            "Epoch 124/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.0000e+00\n",
            "Epoch 125/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.0000e+00\n",
            "Epoch 126/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 127/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 128/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 129/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 130/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 131/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 132/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.0000e+00\n",
            "Epoch 133/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 0.0000e+00\n",
            "Epoch 134/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0453 - accuracy: 0.0000e+00\n",
            "Epoch 135/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 136/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 137/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 138/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 139/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 140/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 141/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 142/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 143/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 144/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 145/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 146/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 147/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 148/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.0000e+00\n",
            "Epoch 149/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 150/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 151/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.0000e+00\n",
            "Epoch 152/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 153/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 154/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
            "Epoch 155/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 156/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 157/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.0000e+00\n",
            "Epoch 158/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 159/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 160/800\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0454 - accuracy: 0.0000e+00\n",
            "Epoch 161/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 162/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 163/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.0000e+00\n",
            "Epoch 164/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 165/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 166/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 167/800\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0450 - accuracy: 0.0000e+00\n",
            "Epoch 168/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0451 - accuracy: 0.0000e+00\n",
            "Epoch 169/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.0000e+00\n",
            "Epoch 170/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 171/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 172/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.0000e+00\n",
            "Epoch 173/800\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 174/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
            "Epoch 175/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 176/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 177/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 178/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
            "Epoch 179/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.0000e+00\n",
            "Epoch 180/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 181/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 182/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 183/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
            "Epoch 184/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
            "Epoch 185/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 186/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0451 - accuracy: 0.0000e+00\n",
            "Epoch 187/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 188/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 189/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 190/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 191/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 192/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.0000e+00\n",
            "Epoch 193/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 194/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.0000e+00\n",
            "Epoch 195/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.0000e+00\n",
            "Epoch 196/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.0000e+00\n",
            "Epoch 197/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 198/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.0000e+00\n",
            "Epoch 199/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 200/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 201/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.0000e+00\n",
            "Epoch 202/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
            "Epoch 203/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.0000e+00\n",
            "Epoch 204/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 205/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 206/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 207/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 208/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.0000e+00\n",
            "Epoch 209/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 210/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
            "Epoch 211/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 212/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 213/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 214/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
            "Epoch 215/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 216/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.0000e+00\n",
            "Epoch 217/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.0000e+00\n",
            "Epoch 218/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 219/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.0000e+00\n",
            "Epoch 220/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.0000e+00\n",
            "Epoch 221/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.0000e+00\n",
            "Epoch 222/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 223/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 224/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 225/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 226/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.0000e+00\n",
            "Epoch 227/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 228/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 229/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.0000e+00\n",
            "Epoch 230/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.0000e+00\n",
            "Epoch 231/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 232/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 233/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 234/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 235/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 236/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
            "Epoch 237/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 238/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.0000e+00\n",
            "Epoch 239/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
            "Epoch 240/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 241/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 242/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
            "Epoch 243/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 244/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.0000e+00\n",
            "Epoch 245/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 246/800\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0457 - accuracy: 0.0000e+00\n",
            "Epoch 247/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.0000e+00\n",
            "Epoch 248/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 249/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
            "Epoch 250/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.0000e+00\n",
            "Epoch 251/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 252/800\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 0.0000e+00\n",
            "Epoch 253/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.0000e+00\n",
            "Epoch 254/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
            "Epoch 255/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.0000e+00\n",
            "Epoch 256/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.0000e+00\n",
            "Epoch 257/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 258/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.0000e+00\n",
            "Epoch 259/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.0000e+00\n",
            "Epoch 260/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
            "Epoch 261/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
            "Epoch 262/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.0000e+00\n",
            "Epoch 263/800\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
            "Epoch 264/800\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.0505 - accuracy: 0.0000e+00"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8373eb7a6ea4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.fit(x,y,epochs=800)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3myaBjY629Z6"
      },
      "outputs": [],
      "source": [
        "model.predict([-3,-2,-1,0,1,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A96eE_TX3nXg"
      },
      "outputs": [],
      "source": [
        "xin = np.array([0,100,1,-2,2,3,-3])\n",
        "yout= model.predict(xin)\n",
        "#orginal output\n",
        "def output(x):\n",
        "  p = 3 * x**3 + 7 * x**2 - 12 * x + 2\n",
        "  return p\n",
        "# print(xin*xmax)\n",
        "for i in range(0,len(xin)):\n",
        "  print(\"input=\",xin[i],end=\"   \")\n",
        "  print(\"predicted_output={0:.2f}    original_output={0:.2f}\".format(round(yout[i][0]),output(xin[i])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0paOgu8-ZXev"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "**Customize ANN**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nv7xt3QCZvuk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mlQDujVHahg3"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3740764125.py, line 6)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    class\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#data normalization\n",
        "(x_train,y_train),(x_test,y_test)=keras.datasets.mnist.load_data()\n",
        "#data normalization\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9GtaajKa-BA",
        "outputId": "2357abf9-0f13-4461-a517-866bbdc9199a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-13 07:42:40.042279: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2023-07-13 07:42:40.042365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: rafz\n",
            "2023-07-13 07:42:40.042448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: rafz\n",
            "2023-07-13 07:42:40.042575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.125.6\n",
            "2023-07-13 07:42:40.042633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.125.6\n",
            "2023-07-13 07:42:40.042652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.125.6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 111,146\n",
            "Trainable params: 111,146\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#simple NN model\n",
        "model = keras.Sequential([\n",
        "    \n",
        "    keras.layers.Flatten(input_shape=(28,28)),#input layer\n",
        "    keras.layers.Dense(128,activation='relu'),#hidden layer with 128 neurons\n",
        "    keras.layers.Dense(64,activation='relu'),#hidden layer with 64 neurons\n",
        "    keras.layers.Dense(32,activation='relu'),#hidden layer with 32 neurons\n",
        "    keras.layers.Dense(10,activation='softmax')#output layer with 10 neurons\n",
        "])\n",
        "\n",
        "model.compile( #model compilation\n",
        "    optimizer = 'adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        "\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht8A9b5Ecn0I",
        "outputId": "622bc35c-0270-42cf-a24c-499b076af579"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-13 07:43:11.253482: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2553 - accuracy: 0.9241\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1082 - accuracy: 0.9673\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0748 - accuracy: 0.9767\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0580 - accuracy: 0.9816\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0479 - accuracy: 0.9846\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0398 - accuracy: 0.9867\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0335 - accuracy: 0.9885\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0309 - accuracy: 0.9895\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0248 - accuracy: 0.9916\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0214 - accuracy: 0.9926\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f360ba89ca0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train,y_train,epochs=10)\n",
        "#model2.fit(augmented_train,epochs = 20,validation_data=augmented_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0__YaZa9do2E",
        "outputId": "2d045a70-fca5-4020-db0b-80333da21590"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-13 07:44:17.597424: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 31360000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0817 - accuracy: 0.9782\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.08168454468250275, 0.9782000184059143]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oiwc2BgrqmDw",
        "outputId": "a3829d9e-79db-45e3-eec0-e9a803406767"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-13 08:26:31.289405: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 31360000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([7.0598277e-11, 2.1346962e-07, 2.3111393e-07, 9.2034242e-07,\n",
              "       3.6069647e-10, 1.6366611e-09, 6.9267222e-12, 9.9999851e-01,\n",
              "       1.3057314e-09, 2.4457833e-08], dtype=float32)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGNZBas3hY3l"
      },
      "source": [
        "**Printing:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "RmaYixqfhd28",
        "outputId": "2bed0404-8baa-4113-d059-c03ce4ff65de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#this prediction return a label\n",
        "predicted = model.predict(x_test)\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(10):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(x_test[i],'gray')\n",
        "  plt.xlabel(\"real=\"+str(y_test[i])+\" , \"+\"predicted=\"+str(np.argmax(predicted[i])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGpHf6aBeAc8"
      },
      "source": [
        "**Data Augmantation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5WDUhKAeCVQ"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,      # Rotate images by up to 10 degrees\n",
        "    zoom_range=0.1,         # Zoom in or out by up to 10%\n",
        "    width_shift_range=0.1,  # Shift images horizontally by up to 10%\n",
        "    height_shift_range=0.1, # Shift images vertically by up to 10%\n",
        ")\n",
        "datagen.fit(x_train.reshape((-1,28,28,1)))\n",
        "augmented_train = datagen.flow(x_train.reshape((-1,28,28,1)),y_train,batch_size=32)\n",
        "\n",
        "datagen.fit(x_train.reshape((-1,28,28,1)))\n",
        "augmented_test = datagen.flow(x_test.reshape((-1,28,28,1)),y_test,batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb0mkAyqvyEI"
      },
      "source": [
        "model with augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k77buMkbloYK"
      },
      "outputs": [],
      "source": [
        "model2 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)),\n",
        "    Dense(128,activation='relu'),\n",
        "    Dense(64,activation='relu'),\n",
        "    Dense(32,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "model2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C7BCchImcMA",
        "outputId": "b51a1e2c-c4d4-4a45-dba2-7ecffff697ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 24s 12ms/step - loss: 0.4868 - accuracy: 0.8496\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2230 - accuracy: 0.9307\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1828 - accuracy: 0.9444\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1610 - accuracy: 0.9506\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.1475 - accuracy: 0.9546\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1370 - accuracy: 0.9582\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1299 - accuracy: 0.9590\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1229 - accuracy: 0.9626\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1169 - accuracy: 0.9640\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1148 - accuracy: 0.9646\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1081 - accuracy: 0.9664\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 23s 13ms/step - loss: 0.1067 - accuracy: 0.9671\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.1014 - accuracy: 0.9685\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0994 - accuracy: 0.9693\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0953 - accuracy: 0.9700\n"
          ]
        }
      ],
      "source": [
        "model2.fit(augmented_train,epochs=15)\n",
        "#model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DmoWTjvpFNi",
        "outputId": "903bb7f5-172b-4498-c2b6-fdcee0386430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 2.2933 - accuracy: 0.1393\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 2.3036 - accuracy: 0.1202\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9816\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0902 - accuracy: 0.9711\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.09017091244459152, 0.9710999727249146]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(x_test,y_test)\n",
        "model.evaluate(augmented_test)\n",
        "\n",
        "model2.evaluate(x_test,y_test)\n",
        "model2.evaluate(augmented_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnW1HmC8qIeB"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMZWhmHPqOK5"
      },
      "outputs": [],
      "source": [
        "from keras.layers.serialization import activation\n",
        "cnn = keras.models.Sequential([\n",
        "    #feature extraction part\n",
        "    keras.layers.Conv2D(32,(3,3),activation='relu',padding='same',input_shape=(28,28,1)),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "    keras.layers.Conv2D(64,(3,3),padding='same',activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    #classification part\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128,activation='relu'),#hidden layer with 128 neurons\n",
        "    keras.layers.Dense(64,activation='relu'),#hidden layer with 64 neurons\n",
        "    keras.layers.Dense(32,activation='relu'),#hidden layer with 32 neurons\n",
        "    keras.layers.Dense(10,activation='softmax')#output layer with 10 neurons\n",
        "    ])\n",
        "\n",
        "\n",
        "cnn.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUqo0meivMoy",
        "outputId": "88858197-76cd-4a54-d399-c05226166a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 26s 13ms/step - loss: 0.2606 - accuracy: 0.9169\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0898 - accuracy: 0.9723\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0650 - accuracy: 0.9797\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0557 - accuracy: 0.9828\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0488 - accuracy: 0.9856\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0450 - accuracy: 0.9861\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0418 - accuracy: 0.9875\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0370 - accuracy: 0.9891\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0363 - accuracy: 0.9891\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0338 - accuracy: 0.9898\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0339 - accuracy: 0.9893\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0317 - accuracy: 0.9905\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0307 - accuracy: 0.9906\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0285 - accuracy: 0.9913\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0267 - accuracy: 0.9921\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7cc1e858fe80>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.fit(augmented_train,epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f8s600ex16i",
        "outputId": "118f4c2a-b481-4a90-c48c-3c805bf58a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3124 - accuracy: 0.0874\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 2.3091 - accuracy: 0.0993\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9816\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0935 - accuracy: 0.9706\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0278 - accuracy: 0.9916\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0308 - accuracy: 0.9910\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.030834194272756577, 0.9909999966621399]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(x_test,y_test)\n",
        "model.evaluate(augmented_test)\n",
        "\n",
        "model2.evaluate(x_test,y_test)\n",
        "model2.evaluate(augmented_test)\n",
        "\n",
        "cnn.evaluate(x_test,y_test)\n",
        "cnn.evaluate(augmented_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-swk7ITzBkz"
      },
      "source": [
        "Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTbdqnPLzFJw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
